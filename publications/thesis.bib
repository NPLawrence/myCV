@phdthesis{lawrence2016ConvexNonconvex,
  title = {Convex and nonconvex optimization techniques for the constrained {{Fermat-Torricelli}} problem},
  author = {Lawrence, Nathan},
  url={https://doi.org/10.15760/honors.319},
  year = {2016},
  urldate = {2020-12-30},
  abstract = {The Fermat-Torricelli problem asks for a point that minimizes the sum of the distances to three given points in the plane. This problem was introduced by the French mathematician Fermat in the 17th century and was solved by the Italian mathematician and physicist Torricelli. In this thesis we introduce a constrained version of the Fermat-Torricelli problem in high dimensions that involves distances to a finite number of points with both positive and negative weights. Based on the distance penalty method, Nesterov's smoothing technique, and optimization techniques for minimizing differences of convex functions, we provide effective algorithms to solve the problem. Attaining numerical results is a work in progress.},
  copyright = {All rights reserved},
  langid = {english},
  school = {Portland State University},
  _venue = {Portland State University}
}

@phdthesis{lawrence2023DeepReinforcement,
  type = {Text},
  title = {Deep reinforcement learning agents for industrial control system design},
  author = {Lawrence, Nathan},
  url={https://dx.doi.org/10.14288/1.0430547},
  year = {2023},
  abstract = {Deep reinforcement learning (RL) is an optimization-driven framework for producing control strategies without explicit reliance on process models. Powerful new methods in RL are often showcased for their performance on difficult simulated tasks In contrast, industrial control system design has many intrinsic features that make "nominal" RL methods unsafe and inefficient. We develop methods for automatic control based on RL techniques while balancing key industrial requirements, such as interpretability, efficiency, and stability. A practical testbed for new control techniques is proportional-integral (PI) control due to its simple structure and prevalence in industry. In particular, PI controllers are elegantly compatible with RL methods as trainable policy "networks". We deploy this idea on a pilot-scale two-tank system, elucidating the challenges in real-world implementation and advantages of our method. To improve the scalability of RL-based controller tuning, we propose an extension based on "meta-RL" wherein a generalized agent is trained for the fast adaptation across a broad collection of dynamics. A key design element is the ability to leverage model-based information offline during training while maintaining a model-free policy structure for interacting with novel processes. Beyond PI control, we propose a framework for the design of feedback controllers that combines the model-free advantages of deep RL with the stability guarantees provided by the Youla-Kucera parameterization to define the search domain. This is accomplished through a data-driven realization of the Youla-Ku\v{c}era parameterization working in tandem with a neural network representation of stable nonlinear operators. Ultimately, our approach is flexible, modular, and decouples the stability requirement from the choice of RL algorithm.},
  langid = {english}
  school = {The University of British Columbia},
 _venue = {The University of British Columbia},
}